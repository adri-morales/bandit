# Multi-Armed Bandits
This repository contains a simple implementation of multi-armed bandits algorithms to explore the basics of reinforcement learning.

## Description
The goal of this project is to explore the basics of reinforcement learning by implementing different algorithms for solving the multi-armed bandits problem. We will start by implementing the epsilon-greedy algorithm, then move on to the upper confidence bound (UCB) algorithm, and finish with a non-stationary bandits problem. This project is intended for beginners to reinforcement learning who want to gain a practical understanding of these algorithms and how they work.

## Contributing
This repository is intended to be a personal project to build a portfolio on reinforcement learning, so contributions are not necessary. However, if you have any suggestions or improvements, feel free to submit a pull request or open an issue.

## License
This project is licensed under the MIT License - see the LICENSE file for details.

## TODO
* Implement non stationary environment
* Automatize workflow with Make
* Include key parameters in history path names
* Implement gradient bandits agent
* compare methods exploring optimal parameter values (alpha, c, Qi)
* Implement exercide 2.4 from Sutton's book